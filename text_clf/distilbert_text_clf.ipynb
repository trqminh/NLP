{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "9JLfJ3K3xIbi",
    "outputId": "25ecf039-052c-4652-f6b6-b3a2249fac1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "e8fZJNsG0ngx",
    "outputId": "e7765242-146f-4abf-80ba-2525fdc6b84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.47)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "oPLQC0Qf0v_K",
    "outputId": "2b7bc5e9-0eb7-43a0-ebb8-a8a962cd292b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertConfig, BertTokenizer, \\\n",
    "DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertModel\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import copy\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger()\n",
    "csv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yomiBuO42JUU",
    "outputId": "39a7771c-1218-4c4c-f2af-6fbbc7984e4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_lines = []\n",
    "with open('/content/drive/My Drive/data/bbc_news/train.tsv', \"r\", encoding=\"utf-8\") as f:\n",
    "  reader = csv.reader(f, delimiter=\"\\t\")\n",
    "  \n",
    "  for line in reader:\n",
    "    if sys.version_info[0] == 2:\n",
    "      line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "    train_lines.append(line)\n",
    "  \n",
    "train_lines = np.array(train_lines)\n",
    "print(train_lines)\n",
    "\n",
    "\n",
    "test_lines = []\n",
    "with open('/content/drive/My Drive/data/bbc_news/test.tsv', \"r\", encoding=\"utf-8\") as f:\n",
    "  reader = csv.reader(f, delimiter=\"\\t\")\n",
    "  \n",
    "  for line in reader:\n",
    "    if sys.version_info[0] == 2:\n",
    "      line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "    test_lines.append(line)\n",
    "  \n",
    "test_lines = np.array(test_lines)\n",
    "print(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pEWuPScYAFg_",
    "outputId": "b01f41c1-5e6c-404f-b7e7-f64357e2c96b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_seq_length = 128\n",
    "label_names = ['entertainment', 'business', 'sport', 'politics', 'tech']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAIwV_8TKwES"
   },
   "outputs": [],
   "source": [
    "class BBCNewsDataset(Dataset):\n",
    "    def __init__(self, text_inputs, labels):\n",
    "        self.all_input_ids = list()\n",
    "        self.text_inputs = text_inputs\n",
    "        for text in self.text_inputs:\n",
    "          tokens = tokenizer.tokenize(text)\n",
    "\n",
    "          if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "          tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "          input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "          padding = [0] * (max_seq_length - len(input_ids))\n",
    "          input_ids += padding\n",
    "\n",
    "          self.all_input_ids.append(input_ids)\n",
    "          \n",
    "        self.labels = le.transform(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.all_input_ids[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "8WbqZeCjAWp6",
    "outputId": "049028aa-7e11-4e09-c5e2-8fabb4db2ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  101,  2093, 23837,  5672, 14113,  2557,  2265,  1996,  2397,  2198,\n",
      "        14113,  1005,  1055,  4035,  2557,  1015,  2265,  2003,  2000,  2022,\n",
      "         4594,  1999,  2337,  2011,  2093,  3065,  4354,  2011,  2093, 23837,\n",
      "         7995,  2006,  7578,  1010,  2512,  1011,  3293,  2189,  1012, 15876,\n",
      "         2860, 15037,  1010, 20710,  6448, 14074,  1998,  6487,  4830,  2924,\n",
      "         2097,  2169,  3677,  1996,  3054,  1011,  2733,  1010,  2397,  1011,\n",
      "         2305,  2335, 10994,  1010, 27696,  2866,  5848,  1012,  2557,  1015,\n",
      "         2056,  1996,  2265,  2052,  2025,  3046,  2000,  5672, 14113,  1010,\n",
      "         2021,  2052,  4125,  2000,  1996,  1000,  4119,  1000,  1997,  1000,\n",
      "         4363,  2010,  8027,  4142,  1000,  2007, 21446,  2189,  1012, 14113,\n",
      "         2351,  2044,  6114,  1037,  2540,  2886,  1999,  7304,  1999,  2255,\n",
      "         1012,  2557,  1015,  2056,  1996,  2093, 23837,  2018,  2042,  4217,\n",
      "         2005,  2037,  1000,  1999,  1011,  5995,  3315,   102]), tensor(1))\n",
      "(tensor([  101,  4511,  4038,  5222,  4401,  1005,  2400, 21864, 15952,  4038,\n",
      "        12579,  2001,  2315,  1996,  2190,  2143,  1997,  1996,  2095,  2011,\n",
      "         1996,  3050,  3349,  2143,  4401,  2523,  1012,  1996,  2149,  3185,\n",
      "         2036,  3856,  2039,  2176,  2060, 27447,  2164,  2190,  2472,  2005,\n",
      "         3656, 13470,  1998,  4637,  3364,  2005,  2726,  2018,  2368,  2277,\n",
      "         1012,  2329,  3883, 10047, 14273,  2050,  2358,  4887, 15104,  2038,\n",
      "         2153,  2042,  7843,  2005,  2014,  2535,  1999, 12297,  7867,  1010,\n",
      "         3045,  2190,  3883,  1010,  2096,  8230,  7663,  3385,  2180,  2190,\n",
      "         3364,  2005, 12631,  7952,  1012,  1996,  2982,  2097,  2022,  4375,\n",
      "         2041,  2006,  2410,  2254,  2012,  1037,  5103,  1999,  5869,  7136,\n",
      "         1012, 12579,  4136,  1996,  2466,  1997,  2048,  2273,  2040,  2202,\n",
      "         1037,  2346,  4440,  2083,  2662,  1005,  1055,  4511,  4655,  1998,\n",
      "         2036,  3340,  2703, 27699, 18900,  3775,  1012,   102]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "train_bbc_news_dataset = BBCNewsDataset(train_lines[:, 0], train_lines[:, 1])\n",
    "test_bbc_news_dataset = BBCNewsDataset(test_lines[:, 0], test_lines[:, 1])\n",
    "print(train_bbc_news_dataset[0])\n",
    "print(test_bbc_news_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6doyc3m23oRz"
   },
   "outputs": [],
   "source": [
    "num_classes = len(label_names)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "train_loader = DataLoader(dataset=train_bbc_news_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_bbc_news_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IJwDDmm44Ii4",
    "outputId": "2c2649e1-b6bd-4646-ba78-4ae702c6bfaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Distilbert model parameter: 3845\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = torch.nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5000, 10000, 15000], gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print('Distilbert model parameter: {}'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h_b0jXwBao49",
    "outputId": "ce75a261-9187-44f5-9d3c-4dd962afe5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1777\n"
     ]
    }
   ],
   "source": [
    "print(len(train_bbc_news_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m6IFtwkTGeq_",
    "outputId": "6505e78d-1750-49c8-bdb3-66bfd3b563c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] Iteration 10 -> Train Loss: 1.5630, train_acc: 0.350\n",
      "[Epoch 1/20] Iteration 20 -> Train Loss: 1.4817, train_acc: 0.375\n",
      "Accuracy on test set after 1 epoch: 63.07%\n",
      "update best\n",
      "----------\n",
      "[Epoch 2/20] Iteration 30 -> Train Loss: 1.4216, train_acc: 0.544\n",
      "[Epoch 2/20] Iteration 40 -> Train Loss: 1.3444, train_acc: 0.723\n",
      "[Epoch 2/20] Iteration 50 -> Train Loss: 1.2815, train_acc: 0.839\n",
      "Accuracy on test set after 2 epoch: 88.59%\n",
      "update best\n",
      "----------\n",
      "[Epoch 3/20] Iteration 60 -> Train Loss: 1.2084, train_acc: 0.868\n",
      "[Epoch 3/20] Iteration 70 -> Train Loss: 1.1669, train_acc: 0.836\n",
      "[Epoch 3/20] Iteration 80 -> Train Loss: 1.1113, train_acc: 0.883\n",
      "Accuracy on test set after 3 epoch: 92.17%\n",
      "update best\n",
      "----------\n",
      "[Epoch 4/20] Iteration 90 -> Train Loss: 1.0430, train_acc: 0.916\n",
      "[Epoch 4/20] Iteration 100 -> Train Loss: 1.0147, train_acc: 0.898\n",
      "[Epoch 4/20] Iteration 110 -> Train Loss: 0.9708, train_acc: 0.928\n",
      "Accuracy on test set after 4 epoch: 94.19%\n",
      "update best\n",
      "----------\n",
      "[Epoch 5/20] Iteration 120 -> Train Loss: 0.9298, train_acc: 0.933\n",
      "[Epoch 5/20] Iteration 130 -> Train Loss: 0.8956, train_acc: 0.917\n",
      "[Epoch 5/20] Iteration 140 -> Train Loss: 0.8512, train_acc: 0.930\n",
      "Accuracy on test set after 5 epoch: 92.85%\n",
      "----------\n",
      "[Epoch 6/20] Iteration 150 -> Train Loss: 0.8230, train_acc: 0.922\n",
      "[Epoch 6/20] Iteration 160 -> Train Loss: 0.7906, train_acc: 0.925\n",
      "Accuracy on test set after 6 epoch: 94.19%\n",
      "----------\n",
      "[Epoch 7/20] Iteration 170 -> Train Loss: 0.7672, train_acc: 0.928\n",
      "[Epoch 7/20] Iteration 180 -> Train Loss: 0.7372, train_acc: 0.933\n",
      "[Epoch 7/20] Iteration 190 -> Train Loss: 0.7203, train_acc: 0.938\n",
      "Accuracy on test set after 7 epoch: 94.41%\n",
      "update best\n",
      "----------\n",
      "[Epoch 8/20] Iteration 200 -> Train Loss: 0.6775, train_acc: 0.935\n",
      "[Epoch 8/20] Iteration 210 -> Train Loss: 0.6748, train_acc: 0.934\n",
      "[Epoch 8/20] Iteration 220 -> Train Loss: 0.6386, train_acc: 0.939\n",
      "Accuracy on test set after 8 epoch: 94.63%\n",
      "update best\n",
      "----------\n",
      "[Epoch 9/20] Iteration 230 -> Train Loss: 0.6344, train_acc: 0.919\n",
      "[Epoch 9/20] Iteration 240 -> Train Loss: 0.5982, train_acc: 0.950\n",
      "[Epoch 9/20] Iteration 250 -> Train Loss: 0.5930, train_acc: 0.933\n",
      "Accuracy on test set after 9 epoch: 95.08%\n",
      "update best\n",
      "----------\n",
      "[Epoch 10/20] Iteration 260 -> Train Loss: 0.5820, train_acc: 0.930\n",
      "[Epoch 10/20] Iteration 270 -> Train Loss: 0.5845, train_acc: 0.914\n",
      "[Epoch 10/20] Iteration 280 -> Train Loss: 0.5370, train_acc: 0.937\n",
      "Accuracy on test set after 10 epoch: 95.08%\n",
      "----------\n",
      "[Epoch 11/20] Iteration 290 -> Train Loss: 0.5281, train_acc: 0.939\n",
      "[Epoch 11/20] Iteration 300 -> Train Loss: 0.5063, train_acc: 0.966\n",
      "Accuracy on test set after 11 epoch: 95.08%\n",
      "update best\n",
      "----------\n",
      "[Epoch 12/20] Iteration 310 -> Train Loss: 0.5249, train_acc: 0.937\n",
      "[Epoch 12/20] Iteration 320 -> Train Loss: 0.4930, train_acc: 0.939\n",
      "[Epoch 12/20] Iteration 330 -> Train Loss: 0.4937, train_acc: 0.933\n",
      "Accuracy on test set after 12 epoch: 95.08%\n",
      "----------\n",
      "[Epoch 13/20] Iteration 340 -> Train Loss: 0.4609, train_acc: 0.953\n",
      "[Epoch 13/20] Iteration 350 -> Train Loss: 0.4625, train_acc: 0.933\n",
      "[Epoch 13/20] Iteration 360 -> Train Loss: 0.4665, train_acc: 0.934\n",
      "Accuracy on test set after 13 epoch: 95.3%\n",
      "update best\n",
      "----------\n",
      "[Epoch 14/20] Iteration 370 -> Train Loss: 0.4638, train_acc: 0.926\n",
      "[Epoch 14/20] Iteration 380 -> Train Loss: 0.4245, train_acc: 0.948\n",
      "[Epoch 14/20] Iteration 390 -> Train Loss: 0.4388, train_acc: 0.934\n",
      "Accuracy on test set after 14 epoch: 95.3%\n",
      "----------\n",
      "[Epoch 15/20] Iteration 400 -> Train Loss: 0.4286, train_acc: 0.937\n",
      "[Epoch 15/20] Iteration 410 -> Train Loss: 0.4134, train_acc: 0.939\n",
      "[Epoch 15/20] Iteration 420 -> Train Loss: 0.4116, train_acc: 0.942\n",
      "Accuracy on test set after 15 epoch: 95.53%\n",
      "update best\n",
      "----------\n",
      "[Epoch 16/20] Iteration 430 -> Train Loss: 0.3926, train_acc: 0.953\n",
      "[Epoch 16/20] Iteration 440 -> Train Loss: 0.3856, train_acc: 0.947\n",
      "Accuracy on test set after 16 epoch: 95.31%\n",
      "----------\n",
      "[Epoch 17/20] Iteration 450 -> Train Loss: 0.3979, train_acc: 0.925\n",
      "[Epoch 17/20] Iteration 460 -> Train Loss: 0.3799, train_acc: 0.936\n",
      "[Epoch 17/20] Iteration 470 -> Train Loss: 0.3830, train_acc: 0.944\n",
      "Accuracy on test set after 17 epoch: 95.51%\n",
      "----------\n",
      "[Epoch 18/20] Iteration 480 -> Train Loss: 0.3656, train_acc: 0.941\n",
      "[Epoch 18/20] Iteration 490 -> Train Loss: 0.3767, train_acc: 0.941\n",
      "[Epoch 18/20] Iteration 500 -> Train Loss: 0.3537, train_acc: 0.948\n",
      "Accuracy on test set after 18 epoch: 95.52%\n",
      "----------\n",
      "[Epoch 19/20] Iteration 510 -> Train Loss: 0.3398, train_acc: 0.956\n",
      "[Epoch 19/20] Iteration 520 -> Train Loss: 0.3308, train_acc: 0.953\n",
      "[Epoch 19/20] Iteration 530 -> Train Loss: 0.3655, train_acc: 0.938\n",
      "Accuracy on test set after 19 epoch: 95.98%\n",
      "update best\n",
      "----------\n",
      "[Epoch 20/20] Iteration 540 -> Train Loss: 0.3340, train_acc: 0.953\n",
      "[Epoch 20/20] Iteration 550 -> Train Loss: 0.3412, train_acc: 0.941\n",
      "[Epoch 20/20] Iteration 560 -> Train Loss: 0.3280, train_acc: 0.942\n",
      "Accuracy on test set after 20 epoch: 95.75%\n",
      "----------\n",
      "Training complete in 151m 44s\n",
      "best acc on test set:  0.9597860063825335\n"
     ]
    }
   ],
   "source": [
    "def accuracy(output, labels):\n",
    "  pred = torch.argmax(output, dim=1)\n",
    "  correct = pred.eq(labels)\n",
    "  return torch.mean(correct.float())\n",
    "\n",
    "total_loss, total_acc = 0, 0\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "epochs = 20\n",
    "itr = 1\n",
    "p_itr = 10\n",
    "\n",
    "# start training\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  for samples, labels in train_loader:\n",
    "    samples, labels = samples.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(samples)[0]\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    total_acc += accuracy(output, labels)\n",
    "    scheduler.step()\n",
    "\n",
    "    if itr % p_itr == 0:\n",
    "      print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, train_acc: {:.3f}'\\\n",
    "            .format(epoch + 1, epochs, itr, total_loss / p_itr, total_acc / p_itr))\n",
    "      \n",
    "      loss_list.append(total_loss / p_itr)\n",
    "      acc_list.append(total_acc / p_itr)\n",
    "      total_loss, total_acc = 0, 0\n",
    "    itr += 1\n",
    "\n",
    "  model.eval()\n",
    "  test_acc = 0.0\n",
    "  for samples, labels in test_loader:\n",
    "    with torch.no_grad():\n",
    "      samples, labels = samples.to(device), labels.to(device)\n",
    "      output = model(samples)[0]\n",
    "      test_acc += accuracy(output, labels)\n",
    "\n",
    "  print('Accuracy on test set after {} epoch: {}%'.format(epoch + 1,\n",
    "                                  round(test_acc.item()*100.0/len(test_loader), 2)))\n",
    "\n",
    "  if (test_acc.item() > best_acc):\n",
    "    best_acc = test_acc.item()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    print('update best')\n",
    "\n",
    "  print('-' * 10)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('best acc on test set: ', best_acc/len(test_loader))\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), '/content/drive/My Drive/distil_model.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bert_distilbert_text_clf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
